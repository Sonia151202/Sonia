# -*- coding: utf-8 -*-
"""final1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jr0plZIvgEi9MsYPTU54SKYaKxtQVioC
"""

!pip install textblob
!pip install vaderSentiment
!pip install rouge-score
!pip install --upgrade torch torchvision torchaudio
!pip install torch==2.0.0
!pip install transformers==4.30.0
!pip install textblob==3.0.0  # or another stable version

# Download the spaCy model first
!python -m spacy download en_core_web_sm
# Then install spaCyTextBlob
!pip install spacytextblob
!pip install spacytextblob==0.1.7

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import os
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

nltk.download('wordnet')
nltk.download('punkt_tab')
nltk.data.path.append('C:\\Users\\91849\\AppData\\Roaming\\nltk_data')

# URL of news articles
base_url = "https://www.thestar.com.my/news/latest/"
responses = []
# Loop over pages 1 to 25
for page_num in range(1, 26):
    url = f"{base_url}?pgno={page_num}#latest"
    print(f"Scraping page: {url}")
    response = requests.get(url)

    if response.status_code == 200:
        print(f"Successfully fetched page {page_num}")
        responses.append(response.content)
    else:
        print(f"Failed to fetch the page {page_num}. Status code: {response.status_code}")

print(f"Fetched {len(responses)} pages.")
# Extracting the headlines and URLs
headlines_list = []
urls_list = []
for response in responses:
    soup = BeautifulSoup(response, "html.parser")

    headlines = soup.find_all("h2", class_="f18")

    for headline in headlines:
        headlines_list.append(headline.get_text(strip=True))
        link = headline.find("a")
        if link and link.get("href"):
            urls_list.append(link.get("href"))
        else:
            urls_list.append(None)
            # Creating a DataFrame
df = pd.DataFrame({
    "Headline": headlines_list,
    "URL": urls_list
})
# Extracting article content
def get_article_content(url):
    try:
        article_response = requests.get(url)

        if article_response.status_code == 200:
            article_soup = BeautifulSoup(article_response.content, "html.parser")
            article_content = article_soup.find("div", class_="row content-holder story-wrapper")

            if article_content:
                paragraphs = article_content.find_all("p")
                article_text = " ".join([para.get_text(strip=True) for para in paragraphs])
                return article_text
            else:
                return None
        else:
            print(f"Failed to fetch content for {url}. Status code: {article_response.status_code}")
            return None
    except Exception as e:
        print(f"Error fetching content for {url}: {e}")
        return None
        # Adding article content to the DataFrame
df["Article Content"] = df["URL"].apply(lambda url: get_article_content(url) if url else None)

df.info()
df.head()

# Preprocessing text content (without stopword removal and lemmatization)
def clean_text_without_stopwords_and_lemmatization(text):
    if not text:
        return None
    # Remove non-alphabetical characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Replace multiple spaces with a single space
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Apply the cleaning function to each row of the 'Article Content' column
df['Preprocessed Article Content Without Stopwords'] = df['Article Content'].apply(clean_text_without_stopwords_and_lemmatization)

# Define the ROUGE scorer
from rouge_score import rouge_scorer # Import the rouge_scorer module

scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

# Function to generate a reference summary using NLTK (extractive summary)
def generate_nltk_reference_summary(article_text):
    if not article_text:
        return None
    # Tokenize the article into sentences
    sentences = nltk.sent_tokenize(article_text)
    # We'll simply use the first 3 sentences as the reference (you can adjust this strategy)
    reference_summary = ' '.join(sentences[:3])  # You can modify how many sentences to use
    return reference_summary

# Step 1: Generate the reference summaries using NLTK for each article in the 'Article Content' column
df['Reference Summary'] = df['Preprocessed Article Content Without Stopwords'].apply(generate_nltk_reference_summary)

# Load the GPT-2 tokenizer and model
gpt2_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2")

# Set the padding token to the eos_token
gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token  # Set pad_token to eos_token

def summarize_with_gpt2(text):
    # Check if text is None and return None if it is
    if text is None:
        return None

    # Preprocess the text if necessary (e.g., adding a prompt)
    input_text = "Summarize: " + text

    # Tokenize the input text (this will pad the sequence if necessary)
    inputs = gpt2_tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True, padding=True)

    # Generate summary (using max_new_tokens to control output length)
    summary_ids = gpt2_model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=50,  # Generate only a maximum of 50 new tokens
        num_return_sequences=1,
        early_stopping=True
    )

    # Decode the generated summary
    summary = gpt2_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Assuming 'df' is your DataFrame and 'Preprocessed Article Content Without Stopwords' is the column you want to summarize
df['GPT-2 Summary'] = df['Preprocessed Article Content Without Stopwords'].apply(summarize_with_gpt2)

# Check the first few rows of the DataFrame with the generated summaries
print(df[['Preprocessed Article Content Without Stopwords', 'GPT-2 Summary']].head())

df.info()

# Function to calculate ROUGE scores for each row
def calculate_rouge_scores(row):
    reference = row['Reference Summary']
    generated_summary = row['GPT-2 Summary']

    # Check if either reference or generated_summary is None
    if reference is None or generated_summary is None:
        return {'rouge1': None, 'rouge2': None, 'rougeL': None}  # Return a dictionary with None values

    # Calculate ROUGE scores if both are not None
    scores = scorer.score(reference, generated_summary)

    return scores

# Apply the function to calculate ROUGE scores for each row and store in a new column
df['ROUGE Scores'] = df.apply(calculate_rouge_scores, axis=1)

# If you want to separate ROUGE scores into specific columns (e.g., rouge1, rouge2, and rougeL), you can do this:
df['ROUGE1'] = df['ROUGE Scores'].apply(lambda x: x['rouge1'].fmeasure if x['rouge1'] else None)  # Handle None values
df['ROUGE2'] = df['ROUGE Scores'].apply(lambda x: x['rouge2'].fmeasure if x['rouge2'] else None)  # Handle None values
df['ROUGEL'] = df['ROUGE Scores'].apply(lambda x: x['rougeL'].fmeasure if x['rougeL'] else None)  # Handle None values

# Calculate the average ROUGE scores for T5, ignoring None values
average_rouge1 = df['ROUGE1'].dropna().mean()
average_rouge2 = df['ROUGE2'].dropna().mean()
average_rougel = df['ROUGEL'].dropna().mean()

# Print the average ROUGE scores
print(f"Average ROUGE-1: {average_rouge1:.4f}")
print(f"Average ROUGE-2: {average_rouge2:.4f}")
print(f"Average ROUGE-L: {average_rougel:.4f}")

df.info()

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer  # Import SentimentIntensityAnalyzer

# Initialize the VADER sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

def get_sentiment_with_vader(text):
    sentiment_score = analyzer.polarity_scores(text)  # Get sentiment scores
    compound_score = sentiment_score['compound']  # The compound score is the overall sentiment

    # Classify the sentiment based on the compound score
    if compound_score > 0.05:
        return 'Positive'
    elif compound_score >= -0.05:
        return 'Neutral'
    else:
        return 'Negative'

# Assuming your DataFrame is named 'df' and it has a 'Preprocessed Article Content Without Stopwords' column
df['Sentiment (VADER)'] = df['Preprocessed Article Content Without Stopwords'].apply(get_sentiment_with_vader)

# Display the resulting DataFrame with Sentiments
print(df[['Preprocessed Article Content Without Stopwords', 'Sentiment (VADER)']])

# Get the count of each sentiment class
sentiment_counts_vader = df['Sentiment (VADER)'].value_counts()

# Print the count of each sentiment class
print("\nSentiment Counts (VADER):")
print(sentiment_counts_vader)

from sklearn.utils import resample

# Separate the classes
df_neutral = df[df['Sentiment (VADER)'] == 'Neutral']
df_positive = df[df['Sentiment (VADER)'] == 'Positive']
df_negative = df[df['Sentiment (VADER)'] == 'Negative']

# Oversample the neutral class
df_neutral_oversampled = resample(df_neutral,
                                  replace=True,     # Allow resampling with replacement
                                  n_samples=len(df_positive),  # Match the largest class size
                                  random_state=42)  # For reproducibility

# Combine all the data
df_balanced = pd.concat([df_positive, df_negative, df_neutral_oversampled])

df_balanced

#classification using randomforest
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import RandomOverSampler
from scipy.sparse import vstack  # Import vstack for combining sparse matrices
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Step 1: Feature extraction using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for efficiency
X = vectorizer.fit_transform(df_balanced['Preprocessed Article Content Without Stopwords'])

# Step 2: Preparing the target labels
y = df_balanced['Sentiment (VADER)']  # Replace with your sentiment column

# Step 3: Handle class imbalance using oversampling
oversampler = RandomOverSampler(random_state=42)
X_balanced, y_balanced = oversampler.fit_resample(X, y)

# Step 4: Split the balanced data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

# Step 5: Initialize and train the Random Forest Classifier
random_forest = RandomForestClassifier(random_state=42)
random_forest.fit(X_train, y_train)

# Step 6: Calculate predictions
y_train_pred = random_forest.predict(X_train)
y_test_pred = random_forest.predict(X_test)

# Step 7: Combine training and test data for overall accuracy
X_combined = vstack([X_train, X_test])  # Vertically stack feature matrices
y_combined = pd.concat([y_train, y_test])  # Combine labels
y_combined_pred = random_forest.predict(X_combined)

# Step 8: Calculate accuracies
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
overall_accuracy = accuracy_score(y_combined, y_combined_pred)

# Step 9: Print all accuracies
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Overall Accuracy: {overall_accuracy:.4f}")

# Step 10: Evaluate and visualize the confusion matrix (for test set)
print("\nClassification Report (Test Data):")
print(classification_report(y_test, y_test_pred))

# Confusion Matrix for Test Data
cm = confusion_matrix(y_test, y_test_pred, labels=y.unique())
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=y.unique(), yticklabels=y.unique(), cbar=False)
plt.title("Confusion Matrix Heatmap (Test Data)")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

import joblib
# Assuming `random_forest` is your trained RandomForestClassifier
# and `vectorizer` is your trained TfidfVectorizer

# Save the Random Forest model
joblib.dump(random_forest, "random_forest_model.pkl")

# Save the TF-IDF vectorizer
joblib.dump(vectorizer, "tfidf_vectorizer.pkl")

print("Model and vectorizer have been saved successfully.")
os.getcwd()